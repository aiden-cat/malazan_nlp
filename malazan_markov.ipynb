{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "from scipy.sparse import dok_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal of this notebook is to learn Markov chains with a real application to the Malazan Book Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"C:/Users/aiden/Dropbox/BOOKS/Malazan Series/SE/Steven Erikson - Malazan Book of the Fallen 10 Books/txt/*\")\n",
    "files = [f for f in files if 'train' in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we read in pre-processed text files of the book, stripping out new lines, and treating punctuation as unique word tokens for prediction (by surrounding each punctuation with a space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts_processed = '|'.join([re.escape(s) for s in string.punctuation])\n",
    "\n",
    "def read_text(f):\n",
    "    ### Purpose of this function is to read in text and do some basic cleaning to produce a list of words\n",
    "    ###  Punctuation are replaced by surrounded by space to incorporate them into the model as distinct tokens\n",
    "    lines = open(f, 'r').readlines()\n",
    "    lines = [re.sub(\"\\n|\\t|\\r\", \" \", l) for l in lines]\n",
    "    lines = [re.sub(rf'({puncts_processed})', r\" \\1 \", l) for l in lines]\n",
    "    return ' '.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in all books\n",
    "book = ' '.join([read_text(f) for f in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_words = [w for w in book.split(\" \") if w != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 32775 unique words in the Malazan Book of the Fallen Series\n"
     ]
    }
   ],
   "source": [
    "unq_words = set(corpus_words)\n",
    "print(f\"There are {len(unq_words)} unique words in the Malazan Book of the Fallen Series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have read in all 10 books and combined them into one long list of sequential words, we need to prepare the transition matrix.  The first step is to generate sequences of `i`, `i + 1` words where we are predicting the probability of the next word based only on  the previous one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate transition matrix\n",
    "k = 1 # adjustable\n",
    "word_sequences = [ ' '.join(corpus_words[i:i+k]) for i, _ in enumerate(corpus_words[:-k]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_count = len(list(set(word_sequences)))\n",
    "transition_matrix = dok_matrix((sets_count, len(unq_words)))\n",
    "distinct_word_seq = list(set(word_sequences))\n",
    "# Construct word to idx and idx to word dictionaries\n",
    "word_idx_dict = {word: i for i, word in enumerate(distinct_word_seq)}\n",
    "idx_word_dict = {i: word for i, word in enumerate(distinct_word_seq)}\n",
    "\n",
    "for i, word in enumerate(word_sequences[:-k]):\n",
    "\n",
    "    word_sequence_idx = k_words_idx_dict[word]\n",
    "    next_word_idx = word_idx_dict[corpus_words[i+k]]\n",
    "    transition_matrix[word_sequence_idx, next_word_idx] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build out transition probability matrix\n",
    "prob_matrix = transition_matrix.copy()\n",
    "prob_matrix = prob_matrix / prob_matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  We built our transition matrix, let's sanity check this by seeing if rake follows anomander at a high probability.  \"Anomander Rake\" is a character in the series and so it should follow at a high probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26725\n",
      "Predicting probability for 'rake' following word 'anomander' with probability [[0.805019305019305]] \n"
     ]
    }
   ],
   "source": [
    "## Let's say, for Anomander Rake, what re see?\n",
    "print(word_idx_dict['anomander']) ## word idx is 10996\n",
    "rake_idx = np.where(prob_matrix[26725]>0.5)[1][0]\n",
    "prob_rake = prob_matrix[26725].reshape(-1,1)[rake_idx].tolist()\n",
    "print(f\"Predicting probability for '{idx_word_dict[rake_idx]}' following word 'anomander' with probability {prob_rake} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next thing we need is to conduct weighted randomly sampling based on a given input word\n",
    "def predict_next_word(word):\n",
    "    word_idx = k_words_idx_dict[word]\n",
    "    pred_idx = np.random.choice( prob_matrix[word_idx].shape[1], p = np.ravel(prob_matrix[word_idx].reshape(-1,1).flatten()))\n",
    "    return idx_word_dict[pred_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we are almost ready!   Predict the next X words given an input word\n",
    "def predict_sequence(seed_word, n = 50):\n",
    "    to_pred = seed_word\n",
    "    prediction = [seed_word]\n",
    "    for i in range(n):\n",
    "        pred = predict_next_word(to_pred)\n",
    "        prediction.append(pred)\n",
    "        to_pred = pred\n",
    "    return ' '.join(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anomander rake a world it there a hawker the coast permanently so desperate flight between them i appreciate perhaps no gods below trull sengar came a hoof and spent lifetimes of the defences of the king s dead or she twisted his beard tell me i mean i ain t use'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## How does text prediction look?\n",
    "predict_sequence(\"anomander\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
